{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Extracting coastlines from Global Coastal Transect System GCTS and Satellite Derived Shorelines System SDSS\n",
    "\n",
    "Run the first few cells to load required functions and jump to the section you're interested in afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"..\\src\")\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "import dask\n",
    "# NOTE: query planning is not implemented in dask_geopandas yet, so we have to set \n",
    "# it to False before we do any dask_geopandas import \n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "\n",
    "from coastlines4shorelines.utils import transect_origins_to_coastline,retrieve_transects_by_roi,shoreline_intersections_to_coastline,filter_sp\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from dask.dataframe.utils import make_meta\n",
    "\n",
    "import dask_geopandas\n",
    "import duckdb\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import pystac\n",
    "import shapely\n",
    "from dotenv import load_dotenv\n",
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "from coastmonitor.geo.geometries import geo_bbox\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_options = {\"account_name\": account_name, \"credential\": sas_token}\n",
    "\n",
    "logging.getLogger(\"azure\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load from STAC catalog\n",
    "\n",
    "Load the transects from our CoCliCo STAC catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog = pystac.Catalog.from_file(\n",
    "    \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(coclico_catalog.get_all_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcts = coclico_catalog.get_child(\"gcts\")\n",
    "gcts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Define a region of interest (ROI) based on a kml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "\n",
    "fiona.drvsupport.supported_drivers[\"KML\"] = \"rw\"\n",
    "kml_fp = pathlib.Path(r\"d:\\FHICS\\ShorelineS\\ROIs\\North_Carolina_Virginia.kml\")\n",
    "roi = gpd.read_file(kml_fp, driver=\"KML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Get all transects for this region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects_roi = retrieve_transects_by_roi(roi, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects_roi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(transects_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Apply transect_origins_to_coastline and explode to get one LineString per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline = (\n",
    "    transects_roi.groupby(\"coastline_id\")\n",
    "    .apply(transect_origins_to_coastline)\n",
    "    .explode()\n",
    "    .reset_index(name=\"geometry\")\n",
    "    .drop(columns=[\"level_1\"])\n",
    ")\n",
    "coastline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline = gpd.GeoDataFrame(coastline, crs=4326)\n",
    "coastline = gpd.overlay(coastline, roi[[\"geometry\"]]).explode(index_parts=False)\n",
    "coastline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Explore the ROI polygon and the cleaned up base coastline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = roi.explore()\n",
    "gpd.GeoDataFrame(coastline, crs=4326).explore(color=\"red\", m=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Load ShorelineMonitor SDS series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = dask_geopandas.read_parquet(\n",
    "    \"az://shorelinemonitor-raw-series/release/2024-04-15/sp_NC.parquet\",\n",
    "    storage_options=storage_options,\n",
    ").compute()\n",
    "sdss = sdss.assign(time=pd.to_datetime(sdss.time).dt.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Implement the filtering function to raw time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_clean = filter_sp(sdss)\n",
    "list(sdss_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Sort on transect name and extract transect name into coastline_id, segment_id and transect_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "    sdss = sdss_clean.sort_values(\"transect_id\")\n",
    "    sdss[[\"coastline_id\", \"segment_id\", \"transect_dist\"]] = (\n",
    "        sdss.transect_id.str.extract(r\"cl(\\d+)s(\\d+)tr(\\d+)\")\n",
    "    )\n",
    "    sdss = sdss.astype(\n",
    "        {\"coastline_id\": int, \"segment_id\": int, \"transect_dist\": int}\n",
    "    )\n",
    "    sdss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Create subsets of sdss for 2010 and 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Ensure your time column is in datetime format\n",
    "sdss['time'] = pd.to_datetime(sdss['time'])\n",
    "\n",
    "# Define the specific date\n",
    "specific_date = pd.Timestamp('2010-01-01')\n",
    "\n",
    "# Filter the GeoDataFrame\n",
    "sdss_2010 = sdss.loc[sdss['time'] == specific_date]\n",
    "m=sdss_2010.geometry.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Try a merge of the OSM smoothed coastline and the points for specific years from sdss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_2010=pd.merge(transects_roi[[\"transect_id\",\"lon\",\"lat\"]],sdss_2010,how=\"left\",on=\"transect_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=pd.merge(transects_roi[[\"transect_id\",\"lon\",\"lat\"]],sdss,how=\"left\",on=\"transect_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "gdf = gpd.GeoDataFrame(merge)\n",
    "gdf['time'] = pd.to_datetime(gdf['time'])\n",
    "\n",
    "# Pivot the data\n",
    "lon_pivot = gdf.pivot(index='time', columns='transect_id', values='lon_y')\n",
    "lat_pivot = gdf.pivot(index='time', columns='transect_id', values='lat_y')\n",
    "\n",
    "# Convert the pivot tables to xarray DataArray\n",
    "lon_xr = xr.DataArray(lon_pivot)\n",
    "lat_xr = xr.DataArray(lat_pivot)\n",
    "\n",
    "# Create a Dataset from the DataArrays\n",
    "ds = xr.Dataset({'lon': lon_xr, 'lat': lat_xr})\n",
    "ds.to_netcdf('test.nc')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define the colormap\n",
    "cmap = plt.cm.get_cmap('rainbow', len(ds.time))\n",
    "\n",
    "# Extract the unique years\n",
    "years = pd.to_datetime(ds.time.values)\n",
    "unique_years = sorted(set(years))\n",
    "\n",
    "for i, year in enumerate(unique_years):\n",
    "    # Select data for the specific year\n",
    "    year_data = ds.sel(time=year)\n",
    "    plt.plot(year_data.lon,year_data.lat)\n",
    "    gdf_year=gpd.GeoDataFrame({\"lon\":year_data.lon,\"lat\":year_data.lat})\n",
    "    # Convert lon and lat to geometry\n",
    "    gdf_year['geometry'] = gpd.points_from_xy(gdf_year.lon, gdf_year.lat)\n",
    "    gdf_year = gdf_year.set_crs(epsg=4326)  # WGS 84\n",
    "\n",
    "plt.xlim(-75.6, -75.4)\n",
    "plt.ylim(35,36\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_year.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.to_datetime(ds.time.values).year.round(0)\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = merge[\"time\"].unique()\n",
    "indx = times.argsort()\n",
    "times[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "xrds =merge_2010.to_xarray()\n",
    "\n",
    "time = pd.date_range(start=\"2000\", end=\"2022\", freq=\"YS\")\n",
    "xrds_t = xrds.expand_dims(timey=time)\n",
    "xrds_t.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrds_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(xrds[\"lon_x\"],xrds[\"lat_x\"],xrds[\"lon_y\"],xrds[\"lat_y\"])\n",
    "plt.xlim([-77,-75])\n",
    "plt.ylim([34,37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrds.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_2010=gpd.GeoDataFrame(merge_2010,geometry=gpd.GeoSeries.from_xy(merge_2010.lon_y,merge_2010.lat_y,crs=4326))\n",
    "c_2010=c_2010.rename(columns={\"lon_y\":\"lon\",\"lat_y\":\"lat\"})\n",
    "coast_2010 = shoreline_intersections_to_coastline(c_2010)\n",
    "coast_2010\n",
    "gpd.GeoDataFrame(geometry=coast_2010,crs=4326).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Show both datasets; notice they are points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Ensure your time column is in datetime format\n",
    "sdss['time'] = pd.to_datetime(sdss['time'])\n",
    "\n",
    "# Define the specific date\n",
    "specific_date = pd.Timestamp('2020-01-01')\n",
    "\n",
    "# Filter the GeoDataFrame\n",
    "sdss_2020 = sdss.loc[sdss['time'] == specific_date]\n",
    "sdss_2020.geometry.explore(color=\"red\", m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Ensure your time column is in datetime format\n",
    "sdss['time'] = pd.to_datetime(sdss['time'])\n",
    "\n",
    "# Define the specific date\n",
    "specific_date = pd.Timestamp('2022-01-01')\n",
    "\n",
    "# Filter the GeoDataFrame\n",
    "sdss_2022 = sdss.loc[sdss['time'] == specific_date]\n",
    "sdss_2022.geometry.explore(color=\"green\", m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(sdss_2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Now connect the dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_2010 = shoreline_intersections_to_coastline(sdss_2010)\n",
    "coastline_2020 = shoreline_intersections_to_coastline(sdss_2020)\n",
    "#list(coastline_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Turn the LineStrings into a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_2010 = gpd.GeoDataFrame(geometry=coastline_2010,crs=4326)\n",
    "coastline_2020 = gpd.GeoDataFrame(geometry=coastline_2020,crs=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Now explore both coastlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = coastline_2010.explore()\n",
    "coastline_2020.explore(color=\"red\", columnn=\"time\", m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Assuming gdf is your GeoPandas GeoDataFrame\n",
    "# Sample data creation (replace this with your actual GeoDataFrame)\n",
    "data = {\n",
    "    'transect_name': ['A', 'B', 'A', 'B'],\n",
    "    'lon': [-123.3656, -123.3657, -123.3656, -123.3657],\n",
    "    'lat': [48.4284, 48.4285, 48.4284, 48.4285],\n",
    "    'time': ['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02']\n",
    "}\n",
    "gdf = gpd.GeoDataFrame(data)\n",
    "gdf['time'] = pd.to_datetime(gdf['time'])\n",
    "\n",
    "# Pivot the data\n",
    "lon_pivot = gdf.pivot(index='time', columns='transect_name', values='lon')\n",
    "lat_pivot = gdf.pivot(index='time', columns='transect_name', values='lat')\n",
    "\n",
    "# Convert the pivot tables to xarray DataArray\n",
    "lon_xr = xr.DataArray(lon_pivot)\n",
    "lat_xr = xr.DataArray(lat_pivot)\n",
    "\n",
    "# Create a Dataset from the DataArrays\n",
    "ds = xr.Dataset({'lon': lon_xr, 'lat': lat_xr})\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lon[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
