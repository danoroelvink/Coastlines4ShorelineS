{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# How to extract the Primary Shoreline-Change Signal per Transect?\n",
    "\n",
    "Many satellite-derived shoreline raw series include multiple shoreline-intersections due to transects spanning narrow coastal features, such as spits or lagoons. This results in noisy time series at trensect level because the series often include observations from multiple shorelines. Additionally, challenging conditions like cloud coverage can lead to incomplete detection of shorelines, causing significant step changes and further noise in the data. To address these issues, a filtering process is applied to extract the primary shoreline-change series from the raw data. \n",
    "\n",
    "## Processing Steps\n",
    "The current workflow extracts the primary shoreline-change signal for a transect by following these steps:\n",
    "\n",
    "1. Determine Maximum Valid Distance:\n",
    "    - Set a default transect length of 2000 meters.\n",
    "    - Reduce the distance in areas where the transect intersects a second time with the OSM coastline (e.g., bays) to exclude observations from secondary shorelines.\n",
    "2. Select Offshore Observations:\n",
    "    - Select one observation per year, that the furthest offshore within the determined maximum distance, as the primary observation.\n",
    "3. Iteratively update the Primary Observations by filtering out observations with large step changes AND mad outlier detection:\n",
    "    - Group observations at the transect level within a maximum step change range of 150 meters.\n",
    "    - Remove observations with large step changes and replace with alternative observations if those exist for the same year.\n",
    "    - Recursively detect and remove outliers using the MAD method while replacing with alternative observations if those exist for the same year. \n",
    "4. Compute Statistics:\n",
    "    - Calculate statistical measures for the primary observations.\n",
    "\n",
    "By following these steps, the primary shoreline-change signal is accurately identified for each transect, ensuring the use of the most relevant and representative data for analysis. This process also provides detailed statistics at the transect level for the primary observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. Use the DynamicMap: By default, the map is set to Namibia. Select your area of interest on the DynamicMap. After selecting the area, proceed to the next step to store the spatial extent in variables and retrieve the data from cloud storage.\n",
    "\n",
    "2. Create Visualization Panels: Run the subsequent cells to create dashboard app that shows the clean series with respect to the raw data. By default, these panels open in a new tab in the browser for stability. If the plot does not display correctly, refresh the tab several times (up to 10 times may be needed).\n",
    "\n",
    "In practice, once you have become familiar with the different visualiation tools, you probably want to explore several areas, so you go back to the DynamicMap and zoom to another region, extract the bounds, retrieve, process and show the data (step 1-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import coastpy\n",
    "import colorcet as cc\n",
    "import dask_geopandas\n",
    "import duckdb\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import pystac\n",
    "import shapely\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from coastmonitor.shorelines.intersection import (\n",
    "    add_transect_statistics,\n",
    "    find_primary_signal_per_transect_group,\n",
    ")\n",
    "sys.path.insert(0, \"../src\")\n",
    "from coastlines4shorelines.utils import retrieve_transects_by_roi\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# NOTE: access tokens to the data are available upon request.\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "account_name = \"coclico\"\n",
    "storage_options = {\"account_name\": account_name, \"credential\": sas_token}\n",
    "\n",
    "# These are the URL's to the STAC catalog that we can use to efficiently index the data\n",
    "COCLICO_STAC_URL = \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    "\n",
    "# Global Coastal Transect System (publicly available and in review)\n",
    "GCTS_COLLECTION_NAME = \"gcts\"\n",
    "\n",
    "# Global Coastal Transect Repository (unreleased; access keys provided upon request). This dataset consists\n",
    "# of GCTS + several other characteristics, such as intersection distance to nearest coastline.\n",
    "GCTR_COLLECTION_NAME = \"gctr\"\n",
    "\n",
    "# ShorelineMonitor Raw Series (unreleased; access keys provided upon request). This dataset consists\n",
    "# ShorelineMonitor Shorlines that are mapped onto the Global Coastal Transect System (Raw Series) that\n",
    "# have a wide range of additional statistics used to filter out the primary, high-quality observations.\n",
    "SM_COLLECTION_NAME = \"shorelinemonitor-raw-series\"\n",
    "\n",
    "# These are the transect columns required for the analysis\n",
    "TRANSECT_COLUMNS = [\n",
    "    \"transect_id\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"bearing\",\n",
    "    \"geometry\",\n",
    "    \"osm_coastline_is_closed\",\n",
    "    \"osm_coastline_length\",\n",
    "    \"utm_epsg\",\n",
    "    \"bbox\",\n",
    "    \"quadkey\",\n",
    "    \"country\",\n",
    "    \"common_country_name\",\n",
    "    \"dist_b0\",\n",
    "    \"dist_b30\",\n",
    "    \"dist_b330\",\n",
    "]\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Read the STAC collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog = pystac.Catalog.from_file(COCLICO_STAC_URL)\n",
    "sm_collection = coclico_catalog.get_child(SM_COLLECTION_NAME)\n",
    "gcts_collection = coclico_catalog.get_child(GCTR_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define a region of interest (ROI) based on a kml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import fiona\n",
    " \n",
    "fiona.drvsupport.supported_drivers[\"KML\"] = \"rw\"\n",
    "roi_dir = pathlib.Path.cwd().parent / \"data\" / \"ROIs\"\n",
    "roi = gpd.read_file(roi_dir / \"SanibelCaptiva.kml\", driver=\"KML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total bounds\n",
    "bounds = roi.total_bounds\n",
    "\n",
    "minx, miny, maxx, maxy = bounds\n",
    "print(minx,miny,maxx, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Create a DuckDB query engine to retrieve data from cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastmonitor.shorelines.intersection import (\n",
    "    clean_raw_series,\n",
    "    compute_diffs,\n",
    "    compute_ols_trend,\n",
    ")\n",
    "\n",
    "sds_ts_engine = coastpy.io.STACQueryEngine(\n",
    "    stac_collection=sm_collection,\n",
    "    storage_backend=\"azure\",\n",
    ")\n",
    "sds_ts = sds_ts_engine.get_data_within_bbox(minx, miny, maxx, maxy)\n",
    "transects_engine = coastpy.io.STACQueryEngine(\n",
    "    stac_collection=gcts_collection, storage_backend=\"azure\", columns=TRANSECT_COLUMNS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects = transects_engine.get_data_within_bbox(minx, miny, maxx, maxy)\n",
    "\n",
    "sds_ts_clean = clean_raw_series(\n",
    "    sds_ts,\n",
    "    transects,\n",
    "    method=\"offshore\",\n",
    "    multi_obs_threshold=17.5,\n",
    "    max_step_change=150,\n",
    "    relative_importance_threshold=0.6,\n",
    ")\n",
    "# filter out the primary observations to get cleaner data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ts_clean_prim = sds_ts_clean[sds_ts_clean[\"obs_is_primary\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects_roi = retrieve_transects_by_roi(roi, storage_options=storage_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects_roi[[\"geometry\"]].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge=pd.merge(transects_roi[[\"transect_id\",\"lon\",\"lat\"]],sds_ts_clean_prim,how=\"left\",on=\"transect_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.groupby(\"transect_id\").time.value_counts().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xr\n",
    "\n",
    "gdf = gpd.GeoDataFrame(merge)\n",
    "gdf['time'] = pd.to_datetime(gdf['time'])\n",
    "\n",
    "# Pivot the data\n",
    "lon_pivot = gdf.pivot(index='time', columns='transect_id', values='lon_y')\n",
    "lat_pivot = gdf.pivot(index='time', columns='transect_id', values='lat_y')\n",
    "\n",
    "# Convert the pivot tables to xarray DataArray\n",
    "lon_xr = xr.DataArray(lon_pivot)\n",
    "lat_xr = xr.DataArray(lat_pivot)\n",
    "\n",
    "# Create a Dataset from the DataArrays\n",
    "ds = xr.Dataset({'lon': lon_xr, 'lat': lat_xr})\n",
    "ds.to_netcdf('test.nc')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sds_ts_clean.time.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Visualize in a small app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastmonitor.visualization.apptools import SpatialDataFrameApp\n",
    "\n",
    "sds_ts_clean_ac = compute_ols_trend(\n",
    "    sds_ts_clean[sds_ts_clean[\"obs_is_primary\"]],\n",
    "    transects,\n",
    "    x=\"time\",\n",
    "    y=\"shoreline_position\",\n",
    ")\n",
    "\n",
    "app = SpatialDataFrameApp(sds_ts_clean_ac, transects, sds_ts_clean)\n",
    "app.create_view()\n",
    "app.view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal] *",
   "language": "python",
   "name": "conda-env-coastal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
