{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Examples how to work with Global Coastal Transect System \n",
    "\n",
    "Run the first few cells to load required functions and jump to the section you're interested in afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"..\\src\")\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "import dask\n",
    "# NOTE: query planning is not implemented in dask_geopandas yet, so we have to set \n",
    "# it to False before we do any dask_geopandas import \n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "\n",
    "from coastlines4shorelines.utils import transect_origins_to_coastline,retrieve_transects_by_roi\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from dask.dataframe.utils import make_meta\n",
    "\n",
    "import dask_geopandas\n",
    "import duckdb\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import pystac\n",
    "import shapely\n",
    "from dotenv import load_dotenv\n",
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "from coastmonitor.geo.geometries import geo_bbox\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_options = {\"account_name\": account_name, \"credential\": sas_token}\n",
    "\n",
    "logging.getLogger(\"azure\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load from STAC catalog\n",
    "\n",
    "Load the transects from our CoCliCo STAC catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog = pystac.Catalog.from_file(\n",
    "    \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(coclico_catalog.get_all_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcts = coclico_catalog.get_child(\"gcts\")\n",
    "gcts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Use a dynamic map to extract data by region of interest\n",
    "\n",
    "The IPyleaflet map below can be used to find the bbox coordinates of a certain region.\n",
    "Zoom to the area where you want to extract data and run the next cell. Please keep in\n",
    "mind to wait 1 second because the map has to be rendered before the coordinates can be\n",
    "extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = 41.735966575868716, -70.10032653808595\n",
    "m.zoom = 9\n",
    "m.layout.height = \"800px\"\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE: Wait for the map to render before you run the next cell\n",
    "\n",
    "rendering the map takes a second, so you need to pause 1 second before running the next cell otherwise you cannot parse the north/west/east/south bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this makes a GeoPandas dataframe from the DynamicMap that is rendered above\n",
    "roi = geo_bbox(m.west, m.south, m.east, m.north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# makes a list of all items (data partitions) in the GCTS STAC catalog\n",
    "items = list(gcts.get_all_items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## The dataset is partitioned into geospatial chunks\n",
    "\n",
    "The dataset is divided into different chunks, that each span a different region of the world. In the next cell\n",
    "we read the spatial extends of each chunk and compose that into a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bboxes = pd.concat([geo_bbox(*i.to_dict()[\"bbox\"]) for i in items])\n",
    "bboxes = bboxes.reset_index(drop=True)\n",
    "bboxes.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Now we can find the bboxes that cover our region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bboxes_roi = gpd.sjoin(bboxes, roi)[bboxes.columns]\n",
    "items_roi = [items[i] for i in bboxes_roi.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_roi[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## The STAC items contain references to where the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrefs = [i.assets[\"data\"].href for i in items_roi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Cloud based data\n",
    "\n",
    "The href that you see below is a url to a cloud bucket with the transects for the area of interest. The prefix \"az://\" is the protocol for Azure cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Reading the transect partitions that span our region of interest \n",
    "\n",
    "We will read the data from cloud storage - but only the data that spans our region of interest (the DynamicMap above). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Dask dataframes are lazy\n",
    "\n",
    "These dataframes are not in memory yet. We still have to trigger the compute (see cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_geopandas.read_parquet(hrefs, storage_options=storage_options).sjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Compute the transects that span our region of interest\n",
    "\n",
    "The transects are not in memory yet. In the next cell we will trigger the retrieval from cloud storage to local client by doing a `ddf.compute()` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects = dask_geopandas.read_parquet(hrefs, storage_options=storage_options)\n",
    "transects_roi = (\n",
    "    transects.sjoin(roi.to_crs(transects.crs)).drop(columns=[\"index_right\"]).compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "transects = dask_geopandas.read_parquet(hrefs, storage_options=storage_options)\n",
    "transects_roi = (\n",
    "    transects.sjoin(roi.to_crs(transects.crs)).drop(columns=[\"index_right\"]).compute()\n",
    ")\n",
    "\n",
    "unique_coastline_names = list(\n",
    "    map(str, transects_roi.transect_id.str.extract(r\"(cl\\d+s\\d+)\")[0].unique())\n",
    ")\n",
    "\n",
    "\n",
    "def add_coastline_name(df):\n",
    "    df[\"coastline_name\"] = df.transect_id.str.extract(r\"(cl\\d+s\\d+)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "meta = make_meta(transects)\n",
    "new_col_meta = pd.DataFrame({\"coastline_name\": pd.Series([], dtype=str)})\n",
    "meta = pd.concat([meta, new_col_meta])\n",
    "\n",
    "transects = transects.map_partitions(add_coastline_name, meta=meta)\n",
    "transects_roi = transects.loc[\n",
    "    transects[\"coastline_name\"].isin(unique_coastline_names)\n",
    "].compute()\n",
    "\n",
    "transects_roi = transects_roi.sort_values(\"transect_id\")\n",
    "transects_roi[[\"coastline_id\", \"segment_id\", \"transect_dist\"]] = (\n",
    "    transects_roi.transect_id.str.extract(r\"cl(\\d+)s(\\d+)tr(\\d+)\")\n",
    ")\n",
    "transects_roi = transects_roi.astype(\n",
    "    {\"coastline_id\": int, \"segment_id\": int, \"transect_dist\": int}\n",
    ")\n",
    "transects_roi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Put everything together in one function that retrieves the transects per area of interest\n",
    "\n",
    "This function below contains everything we have discussed so far. It loads transects that having matching coastlines from a STAC catalog into Python memory for a given area of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "transects_roi = retrieve_transects_by_roi(roi, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Sorting the transects\n",
    "\n",
    "Currently the transects are stored by QuadKey to optimize fast read access by filter pushdown. If we want them sorted by the coastline we can do that as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Compose the transect origins into coastlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline = (\n",
    "    transects_roi.groupby(\"coastline_id\")\n",
    "    .apply(transect_origins_to_coastline)\n",
    "    .dropna()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"geometry\"})\n",
    ")\n",
    "coastline = gpd.GeoDataFrame(coastline, crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tratransects_roi.groupby(\"coastline_id\").get_group(13508)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline.explore(column=\"coastline_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Plus we can handle region of interests that do not span all coastlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "\n",
    "fiona.drvsupport.supported_drivers[\"KML\"] = \"rw\"\n",
    "kml_fp = pathlib.Path(r\"d:\\FHICS\\ShorelineS\\ROIs\\North_Carolina_Virginia.kml\")\n",
    "roi = gpd.read_file(kml_fp, driver=\"KML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects_roi = retrieve_transects_by_roi(roi, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function and explode to get one LineString per row\n",
    "coastline = (\n",
    "    transects_roi.groupby(\"coastline_id\")\n",
    "    .apply(transect_origins_to_coastline)\n",
    "    .explode()\n",
    "    .reset_index(name=\"geometry\")\n",
    "    .drop(columns=[\"level_1\"])\n",
    ")\n",
    "coastline = gpd.GeoDataFrame(coastline, crs=4326)\n",
    "coastline = gpd.overlay(coastline, roi[[\"geometry\"]]).explode(index_parts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = roi.explore()\n",
    "gpd.GeoDataFrame(coastline, crs=4326).explore(color=\"red\", m=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Load ShorelineMonitor SDS series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = dask_geopandas.read_parquet(\n",
    "    \"az://shorelinemonitor-raw-series/release/2024-04-15/sp_NC.parquet\",\n",
    "    storage_options=storage_options,\n",
    ").compute()\n",
    "sdss = sdss.assign(time=pd.to_datetime(sdss.time).dt.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = gpd.GeoSeries.from_xy(\n",
    "      sdss[\"lon\"], sdss[\"lat\"]\n",
    "            ).to_list()\n",
    "cl = gpd.GeoDataFrame(coords)\n",
    "cl.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample to explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "transect_sample = np.random.choice(sdss.transect_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss.loc[sdss[\"transect_id\"] == transect_sample][[\"geometry\", \"time\"]].explore(\n",
    "    column=\"time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sp(sp_raw):\n",
    "    \"\"\"\n",
    "    Function to filter shorelines with specified filtering indicator.\n",
    "    \"\"\"\n",
    "\n",
    "    # set up the indicators\n",
    "    # `sp_clean` will include only shoreline positions that satisfy the indicators\n",
    "    sp_clean = sp_raw[\n",
    "        (sp_raw.sh_sinuosity < 10)\n",
    "        & (~sp_raw.obs_on_shoal)\n",
    "        & (sp_raw.obs_is_primary)\n",
    "        & (sp_raw.tr_is_qa)\n",
    "        & (sp_raw.mdn_offset < 3 * sp_raw.tr_stdev)\n",
    "        & (sp_raw.obs_count >= 5)\n",
    "        & (~sp_raw.obs_is_outlier)\n",
    "    ].copy()\n",
    "\n",
    "    # set up the `sp_clean` table\n",
    "    sp_clean = (\n",
    "        sp_clean[\n",
    "            [\"time\", \"transect_id\", \"shoreline_position_trans\", \"geometry\"]\n",
    "        ]  # columns to be included in the clean tables\n",
    "        .rename(columns=({\"shoreline_position_trans\": \"shoreline_position\"}))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return sp_clean\n",
    "\n",
    "\n",
    "# Implement the filtering function to raw time series (`sp`)\n",
    "sdss_clean = filter_sp(sdss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Function to construct shoreline from SDS series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoreline_intersections_to_coastline(df):\n",
    "    # Ensure df is sorted if not already\n",
    "    df = df.sort_values(by=[\"transect_id\", \"segment_id\", \"transect_dist\"])\n",
    "\n",
    "    # Identify partitions by checking where the difference in transect_dist is not 100\n",
    "    # diff() is NaN for the first row, so we use fillna() to set it to a value that does not equal 100 (e.g., 0)\n",
    "    df[\"partition\"] = (df[\"transect_dist\"].diff().fillna(0) != 100).cumsum()\n",
    "\n",
    "    lines = []\n",
    "    for _, partition_df in df.groupby(\"partition\"):\n",
    "        if len(partition_df) > 1:\n",
    "            coords = gpd.GeoSeries.from_xy(\n",
    "                partition_df[\"lon\"], partition_df[\"lat\"]\n",
    "            ).to_list()\n",
    "\n",
    "            # Check if the coastline is closed and this is the only partition\n",
    "            if (\n",
    "                partition_df.osm_coastline_is_closed.iloc[0]\n",
    "                and len(df[\"partition\"].unique()) == 1\n",
    "            ):\n",
    "                coords.append(\n",
    "                    coords[0]\n",
    "                )  # Add the first point at the end to close the loop\n",
    "\n",
    "            lines.append(LineString(coords))\n",
    "        # Else case can be added if needed to handle single-point partitions\n",
    "\n",
    "    return pd.Series(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "transect_sample = np.random.choice(sdss.transect_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss.loc[sdss[\"transect_id\"] == transect_sample][[\"geometry\", \"time\"]].explore(\n",
    "    column=\"time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
